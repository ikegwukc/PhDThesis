\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{ISL:C1}
\@writefile{toc}{\contentsline {chapter}{Chapter\ 1\hskip 1em\relax Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning}{1}{section.1.1}\protected@file@percent }
\newlabel{eq:function}{{1.1}{1}{Machine Learning}{equation.1.1.1}{}}
\citation{ISL:Chapter5}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Supervised Learning}{2}{subsection.1.1.1}\protected@file@percent }
\newlabel{sec:SupervisedLearning}{{1.1.1}{2}{Supervised Learning}{subsection.1.1.1}{}}
\newlabel{eq:MSE}{{1.2}{2}{Supervised Learning}{equation.1.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Validation Approaches}{3}{subsection.1.1.2}\protected@file@percent }
\newlabel{sec:validationApproach}{{1.1.2}{3}{Validation Approaches}{subsection.1.1.2}{}}
\newlabel{eq:CV}{{1.3}{4}{Validation Approaches}{equation.1.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Network Science}{4}{section.1.2}\protected@file@percent }
\newlabel{sec:NetworkScience}{{1.2}{4}{Network Science}{section.1.2}{}}
\newlabel{eq:adjacencyMatrix}{{1.4}{6}{Network Science}{equation.1.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Analysis of Networks}{6}{subsection.1.2.1}\protected@file@percent }
\newlabel{eq:degree}{{1.5}{6}{Analysis of Networks}{equation.1.2.5}{}}
\newlabel{eq:indegree}{{1.6}{7}{Analysis of Networks}{equation.1.2.6}{}}
\newlabel{eq:outdegree}{{1.7}{7}{Analysis of Networks}{equation.1.2.7}{}}
\newlabel{eq:density}{{1.8}{7}{Analysis of Networks}{equation.1.2.8}{}}
\newlabel{eq:degreeCentrality}{{1.9}{7}{Analysis of Networks}{equation.1.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Information Transfer in Financial Markets}{7}{section.1.3}\protected@file@percent }
\newlabel{IFinFM}{{1.3}{7}{Information Transfer in Financial Markets}{section.1.3}{}}
\citation{Foster1981}
\citation{Baginiski 1987}
\citation{InfoTheoryApplications}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Information Theory }{8}{subsection.1.3.1}\protected@file@percent }
\newlabel{sec:InformationTheory}{{1.3.1}{8}{Information Theory}{subsection.1.3.1}{}}
\citation{IntroToTransferEntropy}
\citation{IntroToTransferEntropy2}
\newlabel{{eq:shannonInfo}}{{1.10}{9}{Information Theory}{equation.1.3.10}{}}
\newlabel{eq:Entropy}{{1.11}{9}{Information Theory}{equation.1.3.11}{}}
\newlabel{eq:CondEntropy}{{1.12}{9}{Information Theory}{equation.1.3.12}{}}
\newlabel{eq:MutalInformation}{{1.13}{9}{Information Theory}{equation.1.3.13}{}}
\citation{MIdiffTE}
\citation{IntroToTransferEntropy}
\citation{InfoTheoryApplications}
\citation{TEBook}
\citation{b359}
\citation{kraskovEstimator}
\citation{EstimatingTE}
\citation{KDE}
\citation{KDE}
\citation{kraskovEstimator}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Estimating Transfer Entropy}{11}{subsection.1.3.2}\protected@file@percent }
\newlabel{intro:estimateTE}{{1.3.2}{11}{Estimating Transfer Entropy}{subsection.1.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel Density Estimator}{11}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kraskov Estimator}{12}{section*.5}\protected@file@percent }
\newlabel{intro:Kraskov}{{1.3.2}{12}{Kraskov Estimator}{section*.5}{}}
\citation{JeffTE}
\citation{JeffTE}
\newlabel{KraskovEquation}{{1.22}{13}{Kraskov Estimator}{equation.1.3.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Additional Estimators}{13}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Figures and Tables}{14}{section.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  This figure contains an example of an undirected graph with 7 nodes and 16 edges. This graph is formed from randomly selecting stock symbol tickers from a set of tickers and forming connections randomly. The purpose of this data is solely to demonstrate the basics of network theory. The nodes are defined as circles and have ticker symbols overlaid on them. Edges are formed between nodes with lines between them. Since this is an undirected graph there is no directional information ergo no distinction as to whether one node is connected to another or vice-versa. The only information represented from an undirected graph is that they are connected. Lastly the amount of connections a node has is referred to as degree. So the node \(FB\) has 6 edges or a degree of 6. \relax }}{14}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ExampleNetwork}{{1.1}{14}{This figure contains an example of an undirected graph with 7 nodes and 16 edges. This graph is formed from randomly selecting stock symbol tickers from a set of tickers and forming connections randomly. The purpose of this data is solely to demonstrate the basics of network theory. The nodes are defined as circles and have ticker symbols overlaid on them. Edges are formed between nodes with lines between them. Since this is an undirected graph there is no directional information ergo no distinction as to whether one node is connected to another or vice-versa. The only information represented from an undirected graph is that they are connected. Lastly the amount of connections a node has is referred to as degree. So the node \(FB\) has 6 edges or a degree of 6. \relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces  This table contains an example of simulated data. This table was formed from randomly selecting stock symbol tickers from a set of tickers. The purpose of this data is solely to demonstrate the basics of network theory. Here the columns and row indicies belong to the selected tickers. The values contain either a 1 to represent if there is a connection between the ticker at a particular row index \(i\) and the ticker of a particular column \(j\) or a 0 if there is no connection. \relax }}{15}{table.caption.11}\protected@file@percent }
\newlabel{tab:ExampleTable}{{1.1}{15}{This table contains an example of simulated data. This table was formed from randomly selecting stock symbol tickers from a set of tickers. The purpose of this data is solely to demonstrate the basics of network theory. Here the columns and row indicies belong to the selected tickers. The values contain either a 1 to represent if there is a connection between the ticker at a particular row index \(i\) and the ticker of a particular column \(j\) or a 0 if there is no connection. \relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  This graph is nearly identical to the graph in Figure \ref  {fig:ExampleNetwork} except this graph is a directed network. This means that it contains directional information between the nodes. This directional information is communicated based on the position of the arrow in a line. For example, the node \(FB\) has an edge to \(MSFT\) however \(MSFT\) does not have an edge to \(FB\). The directional information allows other metrics to be used such as in-degree (which represents the amount of edges directed toward a node) and out-degree (which represents the amount of edges directed away from a node. In this example \(FB\) has an in-degree of 2, an out-degree of 4 and a degree of 6. \relax }}{16}{figure.caption.8}\protected@file@percent }
\newlabel{fig:ExampleNetworkDirected}{{1.2}{16}{This graph is nearly identical to the graph in Figure \ref {fig:ExampleNetwork} except this graph is a directed network. This means that it contains directional information between the nodes. This directional information is communicated based on the position of the arrow in a line. For example, the node \(FB\) has an edge to \(MSFT\) however \(MSFT\) does not have an edge to \(FB\). The directional information allows other metrics to be used such as in-degree (which represents the amount of edges directed toward a node) and out-degree (which represents the amount of edges directed away from a node. In this example \(FB\) has an in-degree of 2, an out-degree of 4 and a degree of 6. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces A nearly identical graph to the graph in Figure \ref  {fig:ExampleNetworkDirected}. This directed graph contains weighted edges where a thick edge represents a high weight value (or strong connection). The smaller the edge weight value the thinner the edge will be. Here \(FB\) has a strong connection to \(AAPL\) whereas \(FB\) has a weak connection to \(AMZN\).\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:ExampleNetworkDirectedWeighted}{{1.3}{17}{A nearly identical graph to the graph in Figure \ref {fig:ExampleNetworkDirected}. This directed graph contains weighted edges where a thick edge represents a high weight value (or strong connection). The smaller the edge weight value the thinner the edge will be. Here \(FB\) has a strong connection to \(AAPL\) whereas \(FB\) has a weak connection to \(AMZN\).\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces  A nearly identical graph to the graph in Figure \ref  {fig:ExampleNetworkDirected}. Here the node sizes are scaled based on the degree. The higher the degree the larger the node size and consequently the smaller the degree the smaller the node size. \relax }}{18}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ExampleNetworkDegree}{{1.4}{18}{A nearly identical graph to the graph in Figure \ref {fig:ExampleNetworkDirected}. Here the node sizes are scaled based on the degree. The higher the degree the larger the node size and consequently the smaller the degree the smaller the node size. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces  This figure shows how a data set can be partitioned for a supervised machine learning paradigm. The ``Data Set" is partitioned into 2 unequal sets with the``Train Set" having more data assigned to it than the ``Test Set". The ``Train Set" and ``Test Set" ratios can vary and is typically determined by the practitioner. There are many ways to partition the data for the train and test sets. For example the dataset can be partitioned sequentially from the data, or form partitions based on the data being randomly sampled. \relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig:TrainTestSplit}{{1.5}{19}{This figure shows how a data set can be partitioned for a supervised machine learning paradigm. The ``Data Set" is partitioned into 2 unequal sets with the``Train Set" having more data assigned to it than the ``Test Set". The ``Train Set" and ``Test Set" ratios can vary and is typically determined by the practitioner. There are many ways to partition the data for the train and test sets. For example the dataset can be partitioned sequentially from the data, or form partitions based on the data being randomly sampled. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces  ``Train Set" here represents ``Train Set" in Figure \ref  {fig:TrainTestSplit}. In this figure the data is split into \(k\) folds. How the \(k\) folds are created can vary. A common technique is to randomly divide the ``Train Set" observations into \(k\) equal folds. \relax }}{20}{figure.caption.13}\protected@file@percent }
\newlabel{fig:KFoldValidation}{{1.6}{20}{``Train Set" here represents ``Train Set" in Figure \ref {fig:TrainTestSplit}. In this figure the data is split into \(k\) folds. How the \(k\) folds are created can vary. A common technique is to randomly divide the ``Train Set" observations into \(k\) equal folds. \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces  This figure breaks down how \(k\)-Fold Cross Validation works. These folds are created from ``Train Set" in Figure \ref  {fig:KFoldValidation}. For the first iteration (where you see 1 underlined), the 1st fold (circle labeled 1) is used to assess the models ability and the remaining folds (squares) are used to train the model. The 2nd iteration follows the same procedure as the 1st except only the 2nd fold is withheld and the other folds are used. This repeats \(k\) times. \relax }}{21}{figure.caption.14}\protected@file@percent }
\newlabel{fig:KFoldValidationExplain}{{1.7}{21}{This figure breaks down how \(k\)-Fold Cross Validation works. These folds are created from ``Train Set" in Figure \ref {fig:KFoldValidation}. For the first iteration (where you see 1 underlined), the 1st fold (circle labeled 1) is used to assess the models ability and the remaining folds (squares) are used to train the model. The 2nd iteration follows the same procedure as the 1st except only the 2nd fold is withheld and the other folds are used. This repeats \(k\) times. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces  This figure shows an example of a Decision Tree. This is essentially a graph with no cycles. Each circle represents a node (the label of the node is a letter inside of the node) and the arrows represent directional edges (branches) to another node. Node \(A\) is the root node of this tree and is where the decision begins. Potential outcomes from the root node lead to different states that are represented in nodes \(B\) and \(C\). \(B\) and \(C\) are the children of \(A\). Nodes \(B\) and \(C\) have children and their children has children. Nodes \(H, I, E, J, K,\) and \(L\) are the leaf nodes of this tree because they have no children. \relax }}{22}{figure.caption.15}\protected@file@percent }
\newlabel{fig:DecisionTree}{{1.8}{22}{This figure shows an example of a Decision Tree. This is essentially a graph with no cycles. Each circle represents a node (the label of the node is a letter inside of the node) and the arrows represent directional edges (branches) to another node. Node \(A\) is the root node of this tree and is where the decision begins. Potential outcomes from the root node lead to different states that are represented in nodes \(B\) and \(C\). \(B\) and \(C\) are the children of \(A\). Nodes \(B\) and \(C\) have children and their children has children. Nodes \(H, I, E, J, K,\) and \(L\) are the leaf nodes of this tree because they have no children. \relax }{figure.caption.15}{}}
\bibstyle{plainnat}
\bibdata{thesisbib}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}References}{23}{section.1.5}\protected@file@percent }
\@setckpt{1-introduction}{
\setcounter{page}{24}
\setcounter{equation}{22}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{11}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{r@tfl@t}{0}
\setcounter{lstnumber}{1}
\setcounter{FancyVerbLine}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
